---
title: "LLaVA-OneVision: Easy Visual Task Transfer"
description: "The first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video"
publishDate: "2024-08-05"
mainTags: ["models"]
tags:
  [
    "vision",
    "multimodal",
    "research",
    "llava",
    "video",
    "multi-image",
    "transfer-learning",
  ]
thumbnail: "/images/blog_thumbnails/llava_onevision.png"
authors:
  - name: "Bo Li"
    main: true
  - name: "Yuanhan Zhang"
    main: true
  - name: "Dong Guo"
  - name: "Renrui Zhang"
  - name: "Feng Li"
  - name: "Hao Zhang"
  - name: "Kaichen Zhang"
  - name: "Yanwei Li"
  - name: "Ziwei Liu"
  - name: "Chunyuan Li"
acknowledgement: "This work is a collaboration between researchers from ByteDance, NTU, CUHK, and HKUST, building upon the strong foundation of the LLaVA project series."
bibtex: "@article{li2024llava-onevision,\n  title={LLaVA-OneVision: Easy Visual Task Transfer},\n  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},\n  journal={arXiv preprint arXiv:2408.03326},\n  year={2024}\n}"
---

import { ResponsiveImage, ResourceCard } from "@/components/mdx/components";

<ResponsiveImage
  src="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/demos/fig1.png"
  alt="LLaVA-OneVision"
  maxWidth="100%"
  caption="LLaVA-OneVision: A unified model for single-image, multi-image, and video understanding"
/>

## Overview

We present **LLaVA-OneVision**, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. LLaVA-OneVision is the **first single model** that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: **single-image**, **multi-image**, and **video** scenarios.

## Key Features

### Unified Architecture

LLaVA-OneVision is designed to have a similar maximum visual token count across different scenarios, enabling flexible extension to multiple visual signal types while maintaining consistent performance.

### Model Sizes

- **0.5B parameters** - Lightweight deployment
- **7B parameters** - Balanced performance
- **72B parameters** - State-of-the-art capabilities

## Emerging Capabilities

The design of LLaVA-OneVision enables strong transfer learning across different modalities and scenarios, yielding impressive emerging capabilities:

### 1. Cross-Scenario Understanding

Seamlessly process and understand content across single images, multiple images, and videos within a unified framework.

### 2. Advanced Visual Analysis

- **Diagram and table interpretation** - Understanding complex visual structures
- **Multi-screenshot interaction** - Analyzing relationships across multiple screens
- **Set-of-mark object referencing** - Precise object identification and tracking

### 3. Video Capabilities

- **Image-to-video generation understanding** - Comprehending temporal transitions
- **Video analysis and comparison** - Deep understanding of video content
- **Multi-camera video interpretation** - Processing footage from multiple viewpoints
- **Detailed video subject description** - Rich, contextual video narration

## Strong Transfer Learning

Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos, showcasing the model's ability to generalize learned representations across visual domains.

<ResourceCard
  title="Open-Source Resources"
  description="Complete LLaVA-OneVision resources to facilitate future development of LMMs in the community"
  resources={[
    {
      type: "github",
      title: "Training Code",
      description:
        "Cook a SOTA model with our released training code and reproduction scripts",
      url: "https://github.com/LLaVA-VL/LLaVA-NeXT",
    },
    {
      type: "model",
      title: "Model Checkpoints",
      description:
        "Access pre-trained model checkpoints in all three sizes (0.5B, 7B, 72B)",
      url: "https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37",
    },
    {
      type: "dataset",
      title: "Training Datasets",
      description:
        "Explore comprehensive training datasets for Single-Image and OneVision stages",
      url: "https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data",
    },
    {
      type: "demo",
      title: "Live Demo",
      description: "Try LLaVA-OneVision directly in your browser",
      url: "https://llava-vl.github.io/blog/2024-08-05-llava-onevision/",
    },
  ]}
/>

## Development Roadmap

LLaVA-OneVision represents a significant milestone in our iterative improvements through the LLaVA-NeXT series, focusing on:

- Enhanced reasoning capabilities
- Improved OCR performance
- Expanded world knowledge
- Advanced multimodal understanding
