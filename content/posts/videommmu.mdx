---
title: "Video-MMMU: Evaluating Knowledge Acquisition from Educational Videos"
description: "A benchmark that asks: If a model 'goes to class,' can it learn from lectures and apply knowledge to MMMU-style exam problems?"
publishDate: "2025-01-13"
mainTags: ["benchmarks"]
tags:
  [
    "benchmarks",
    "evaluation",
    "multimodal"
  ]
thumbnail: "/images/blog_thumbnails/videommmu.png"
authors:
  - name: "Kairui Hu"
    main: true
  - name: "Penghao Wu"
    main: true
  - name: "Fanyi Pu"
  - name: "Wang Xiao"
  - name: "Xiang Yue"
  - name: "Yuanhan Zhang"
  - name: "Bo Li"
  - name: "Ziwei Liu"
bibtex: "@article{hu2025videommmu,\n    title={Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos},\n    author={Kairui Hu and Penghao Wu and Fanyi Pu and Wang Xiao and Yuanhan Zhang and Xiang Yue and Bo Li and Ziwei Liu},\n    journal={arXiv preprint arXiv:2501.13826},\n    year={2025},\n    url={https://arxiv.org/abs/2501.13826}\n}"
---

import { ResponsiveImage, ResourceCard, CodeDemo } from "@/components/mdx/components";

<ResponsiveImage
  src="/images/videommmu_images/figure1.png"
  alt="Video-MMMU Overview"
  maxWidth="100%"
  caption="Video-MMMU: A comprehensive benchmark for evaluating knowledge acquisition from educational videos across multiple disciplines"
/>

<div style="text-align: center; margin: 2rem 0;">
  <a
    href="https://videommmu.github.io/"
    style="display: inline-block; margin: 0 6px;"
  >
    <img
      src="https://img.shields.io/badge/üéì-Website-red"
      height="23"
      alt="Website"
    />
  </a>
  <a
    href="https://arxiv.org/abs/2501.13826"
    style="display: inline-block; margin: 0 6px;"
  >
    <img
      src="https://img.shields.io/badge/üìù-Paper-blue"
      height="23"
      alt="Paper"
    />
  </a>
  <a
    href="https://huggingface.co/datasets/lmms-lab/VideoMMMU"
    style="display: inline-block; margin: 0 6px;"
  >
    <img
      src="https://img.shields.io/badge/ü§ó-Dataset-yellow"
      height="23"
      alt="Dataset"
    />
  </a>
</div>

> **Video-MMMU asks a fundamental question:**
> If a model 'goes to class,' can the model learn from the lecture and apply what it learned to MMMU-style exam problems?

## üéØ Motivation

Our original goal was to build a **video reasoning benchmark**, motivated by the observation that the most demanding forms of reasoning arise in **academic settings**‚Äîfor example, MMMU-style university exam questions.

**Online lectures** create an ideal environment for evaluating video reasoning. They effectively convey knowledge and naturally test a model's ability to learn from video. These videos have three key attributes:

1. **High information density** (heavy OCR/ASR signals)
2. **Advanced knowledge requirements** (college-level knowledge)
3. **Temporal structure** (concepts unfolding over time)

These properties make reasoning from lecture video notably harder. This leads to our core question:

**When a model watches an online lecture, can it learn like a student‚Äîunderstand the content, acquire the knowledge, and then solve related problems?**

Therefore, we introduce **Video-MMMU**, a video reasoning benchmark that evaluates **knowledge acquisition from video**.

## üèÜ Video-MMMU Leaderboard

| Model                                                                                    | Overall \\ | Œîknowledge | Perception | Comprehension | Adaptation |
| ---------------------------------------------------------------------------------------- | ---------- | ---------- | ---------- | ------------- | ---------- |
| [GPT-5-thinking](https://openai.com/index/introducing-gpt-5/)                            | 84.6 \\    | --         | --         | --            | --         |
| [Gemini-2.5-Pro](https://deepmind.google/models/gemini/pro/)                             | 83.6 \\    | --         | --         | --            | --         |
| [OpenAI O3](https://openai.com/index/introducing-o3-and-o4-mini/)                        | 83.3 \\    | --         | --         | --            | --         |
| [Claude-3.5-Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)                    | 65.78 \\   | üü¢ +11.4   | 72.00      | 69.67         | 55.67      |
| [Kimi-VL-A3B-Thinking-2506](https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506) | 65.22 \\   | üü¢ +3.5    | 75.00      | 66.33         | 54.33      |
| [GPT-4o](https://openai.com/index/hello-gpt-4o/)                                         | 61.22 \\   | üü¢ +15.6   | 66.00      | 62.00         | 55.67      |
| [Qwen-2.5-VL-72B](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct)                   | 60.22 \\   | üü¢ +9.7    | 69.33      | 61.00         | 50.33      |

_See full leaderboard with 20+ models in our [paper](https://arxiv.org/abs/2501.13826) and [website](https://videommmu.github.io/)_

## üìö Overview

We introduce **Video-MMMU**, a multi-modal, multi-disciplinary, multi-track benchmark designed to evaluate how effectively large multimodal models (LMMs) **acquire knowledge** from educational videos.

### 1) Video: Knowledge Source

Traditional VideoQA benchmarks focus on scene understanding. **Video-MMMU treats video as a source of knowledge**, evaluating whether LMMs can actually _learn_ from instructional content.

**Dataset Composition:**

- **300 college-level, lecture-style videos**
- **30 subjects across 6 disciplines:** Art, Business, Science, Medicine, Humanities, and Engineering
- **High-quality educational content** from university-level courses

### 2) QA Design: Three Stages of Knowledge Acquisition

Each video is paired with **three questions**, designed to reflect a progression in knowledge acquisition:

- **üîç Perception** ‚Äì Identifying relevant surface information
- **üß† Comprehension** ‚Äì Understanding underlying concepts or strategies
- **üéØ Adaptation** ‚Äì Applying learned knowledge to new scenarios

<ResponsiveImage
  src="/images/videommmu_images/figure2.png"
  alt="Knowledge Acquisition Categories"
  maxWidth="100%"
  caption="Figure 2: Examples for each knowledge acquisition category across different disciplines. Perception (ASR/OCR-based), Comprehension (concept/strategy understanding), and Adaptation (application to new scenarios)."
/>

<ResponsiveImage
  src="/images/videommmu_images/figure3.png"
  alt="Benchmark Structure"
  maxWidth="100%"
  caption="Figure 3: Video-MMMU benchmark structure showing the progression from video content to three-tier evaluation framework."
/>

### 3) In-Context Knowledge Acquisition: Learning Like Humans

**Humans consistently learn from the world around them.** For models to operate effectively in real-world environments, the same principle should apply: they must be able to learn from the world, because unlike humans, they cannot be endlessly re-trained after deployment.

In this sense, **videos provide a natural proxy for the world**. For a model, the video becomes its world. The ability to learn from video therefore becomes more than a technical benchmark‚Äîit is a measure of **true, dynamic intelligence**. It marks the shift from simply solving a task to demonstrating the ability to **learn how to solve the task**.

### 4) Metric: From Absolute Accuracy to Learning Efficiency (Œîknowledge)

A core innovation in Video-MMMU is its shift from measuring only final performance to **measuring learning**.

#### Œîknowledge Formula

<CodeDemo title="" showCopy={false}>
```
Œîknowledge = (Acc_after_video - Acc_before_video) / (100% - Acc_before_video) √ó 100%
```
</CodeDemo>

#### Evaluation Process

**1. Initial Test:**
The model attempts to answer a question _without_ seeing the video.

**2. Re-Test after video viewing:**
We provide the corresponding lecture video. The model is asked the same question again.

**3. Performance Gain:**
If the model succeeds after watching, it demonstrates successful knowledge acquisition from video.

This setup mirrors a human's natural educational process:

<CodeDemo title="" showCopy={false}>
```
Don't know ‚Üí Learn by watching ‚Üí Apply the knowledge
```
</CodeDemo>

## üîç Key Insights

<ResponsiveImage
  src="/images/videommmu_images/figure4.png"
  alt="Performance Analysis"
  maxWidth="100%"
  caption="Figure 4: Comprehensive analysis showing progressive performance decline and the human-model gap in knowledge acquisition from videos."
/>

### **Progressive Performance Decline**

Model performance decreases as cognitive demands increase. While models perform relatively better on _Perception_ tasks, accuracy drops on _Comprehension_ and declines further on _Adaptation_.

### **Knowledge Acquisition Challenge**

The Œîknowledge metric reveals a **significant human‚Äìmodel gap**:

- **Humans:** Substantial improvement (Œîknowledge ‚âà **33.1%**)
- **Top Models:** Smaller gains (GPT-4o: **15.6%**, Claude-3.5-Sonnet: **11.4%**)

This highlights a current limitation: **LMMs still struggle to learn from videos** in the way humans do.

## üìä Case Studies

### Failure Case: Method Adaptation Error

<ResponsiveImage
  src="/images/videommmu_images/wrong.png"
  alt="Failure Case Study"
  maxWidth="100%"
  caption="Figure 5: Example of method adaptation failure - the model failed to adapt the method from video to solve the Adaptation question."
/>

### Success Case: Learning from Video

<ResponsiveImage
  src="/images/videommmu_images/wrongtoright.png"
  alt="Success Case Study"
  maxWidth="100%"
  caption="Figure 6: Example of successful learning from video - transforming an initial wrong answer into a correct one after watching the educational content."
/>

## üöÄ Research Impact

### Paradigm Shift

Video-MMMU represents a **paradigm shift** from traditional video understanding to **knowledge acquisition evaluation**:

- **From Scene Understanding to Learning** - Moving beyond visual comprehension to knowledge acquisition
- **From Static Evaluation to Dynamic Learning** - Measuring improvement rather than just final performance
- **From Task Solving to Learning Capability** - Evaluating the ability to learn new skills

### Implications for AI Development

1. **Real-World Deployment** - Models must learn continuously after deployment
2. **Educational AI** - Critical for AI tutoring and educational applications
3. **Knowledge Transfer** - Understanding how models generalize learned concepts
4. **Human-AI Alignment** - Bridging the gap in learning capabilities

## üìà Future Directions

### Benchmark Extensions

- **Multimodal Knowledge Sources** - Incorporating diverse educational formats
- **Long-term Learning** - Evaluating knowledge retention over time
- **Interactive Learning** - Adding feedback loops and iterative improvement

### Model Development

- **Learning-Optimized Architectures** - Designing models specifically for knowledge acquisition
- **Memory Integration** - Better mechanisms for knowledge storage and retrieval
- **Transfer Learning** - Improving cross-domain knowledge application

<ResourceCard
  title="Video-MMMU Resources"
  description="Complete benchmark resources including dataset, evaluation tools, research paper, and project information"
  resources={[
    {
      type: "link",
      title: "Project Website",
      description: "Comprehensive benchmark information",
      url: "https://videommmu.github.io/",
    },
    {
      type: "paper",
      title: "Research Paper",
      description: "Detailed methodology and analysis",
      url: "https://arxiv.org/abs/2501.13826",
    },
    {
      type: "dataset",
      title: "Video-MMMU Dataset",
      description: "Complete benchmark dataset",
      url: "https://huggingface.co/datasets/lmms-lab/VideoMMMU",
    },
    {
      type: "github",
      title: "GitHub Repository",
      description: "Evaluation code and tools",
      url: "https://github.com/EvolvingLMMs-Lab/VideoMMMU",
    },
  ]}
/>

## üéØ Getting Started

1. **Download** the Video-MMMU dataset from Hugging Face
2. **Set up** the evaluation environment using our GitHub repository
3. **Run** baseline evaluations on your models
4. **Analyze** Œîknowledge metrics to understand learning capabilities
5. **Compare** results with our comprehensive leaderboard

**Video-MMMU** challenges the current state of multimodal AI by shifting focus from static performance to **dynamic learning capability** - a critical step toward truly intelligent and adaptive AI systems.
